{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21ab40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kswierkot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kswierkot/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kswierkot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kswierkot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from urllib import request\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import os, ssl\n",
    "import re\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e55508",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dem_platform_2016 = 'https://www.presidency.ucsb.edu/documents/2016-democratic-party-platform'\n",
    "url_debate_las_vegas_2016 = 'https://www.presidency.ucsb.edu/documents/presidential-debate-the-university-nevada-las-vegas'\n",
    "url_debate_st_louis_2016 = 'https://www.presidency.ucsb.edu/documents/presidential-debate-washington-university-st-louis-missouri'\n",
    "url_rep_platform_2016 = 'https://www.presidency.ucsb.edu/documents/2016-republican-party-platform'\n",
    "url_dem_platform_1980 = 'https://www.presidency.ucsb.edu/documents/1980-democratic-party-platform'\n",
    "url_rep_platform_1980 = 'https://www.presidency.ucsb.edu/documents/republican-party-platform-1980'\n",
    "url_debate_cleveland_1980 = 'https://www.presidency.ucsb.edu/documents/presidential-debate-cleveland'\n",
    "url_debate_baltimore_1980 = 'https://www.presidency.ucsb.edu/documents/presidential-debate-baltimore-reagan-anderson'\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "en_stop = en_stop.union(set(['republican', 'democratic']))\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42149543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "html_dem_platform_2016 = request.urlopen(url_dem_platform_2016).read().decode('utf8')\n",
    "raw_dem_platform_2016 = BeautifulSoup(html_dem_platform_2016, 'html.parser')\n",
    "raw_dem_platform_2016 = [p.get_text() for p in raw_dem_platform_2016.select('div.field-docs-content > p')]\n",
    "raw_dem_platform_2016 = ' '.join(raw_dem_platform_2016)\n",
    "raw_dem_platform_2016 = re.sub(r'\\d+', '', raw_dem_platform_2016)\n",
    "\n",
    "html_rep_platform_2016 = request.urlopen(url_rep_platform_2016).read().decode('utf8')\n",
    "raw_rep_platform_2016 = BeautifulSoup(html_rep_platform_2016, 'html.parser')\n",
    "raw_rep_platform_2016 = [p.get_text() for p in raw_rep_platform_2016.select('div.field-docs-content > p')]\n",
    "raw_rep_platform_2016 = ' '.join(raw_rep_platform_2016)\n",
    "raw_rep_platform_2016 = re.sub(r'\\d+', '', raw_rep_platform_2016)\n",
    "\n",
    "data_dem_platform_2016 = []\n",
    "for i in sent_tokenize(raw_dem_platform_2016):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "        \n",
    "    data_dem_platform_2016.append(temp)\n",
    "\n",
    "    \n",
    "data_rep_platform_2016 = []\n",
    "for i in sent_tokenize(raw_rep_platform_2016):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "        \n",
    "    data_rep_platform_2016.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08f3deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model\n",
    "model_dem_cbow = gensim.models.Word2Vec(data_dem_platform_2016, min_count = 1,\n",
    "                              vector_size = 50, window = 1)\n",
    " \n",
    "# Create Skip Gram model\n",
    "model_dem_gram = gensim.models.Word2Vec(data_dem_platform_2016, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59e43969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x12c122dd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dem_cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96217c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c72f385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cosine_similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / np.dot(norm_u, norm_v)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b88c62d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908463"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = cosine_similarity(model_dem_cbow.wv['democrats'], model_dem_cbow.wv['also'])\n",
    "cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "361cd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: complete_analogy\n",
    "\n",
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[w] - e_c))\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a259b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = complete_analogy(*triad,word_to_vec_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
